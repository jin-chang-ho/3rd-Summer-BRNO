{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Homework_ML_basics_S.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"SVnn1fn-qg_W"},"source":["# ML basics - Homework"]},{"cell_type":"code","metadata":{"id":"ji5N3HIlqcyR"},"source":["#######################\n","###### TODO FILL ######\n","#######################\n","\n","#@title Author { run: \"auto\" }\n","NAME = \"\" #@param {type: \"string\"}\n","EMAIL = \"\" #@param {type: \"string\"}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s5cVE8Yk-EYI"},"source":["## How to submit\n","* Create a **copy** of this notebook. \n","* Finish the assigment.\n","* Export the notebook to `.pdf` (File > Print).\n","* Send it to xnguye16@stud.fit.vutbr.cz (Add the pdf in the attachment)."]},{"cell_type":"markdown","metadata":{"id":"NAloFyXHfuJx"},"source":["## Import libraries"]},{"cell_type":"code","metadata":{"id":"dQHE51ZxrDXW"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import scipy.stats as stats\n","from sklearn.metrics import f1_score"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p5MgUTNmB_GC"},"source":["## Download the dataset"]},{"cell_type":"code","metadata":{"id":"lYJccjIVCCHe"},"source":["#!pip install gdown\n","!gdown https://drive.google.com/uc?id=1LdKFD55N4A72qTx1IK6LDW5Mo7L1g0uz\n","!unzip data.zip"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tA2wERcof4pM"},"source":["## Dataset loading\n","\n","First task of this assigment is to load the dataset. Each sample of the dataset consists of 7 features $\\mathbf{x}$ and a label $y$, thus a sample is a tuple $(\\mathbf{x}, y)$, where $\\mathbf{x} = \\begin{bmatrix}\n","  x_1 & x_2 & x_3 & ... & x_7\\end{bmatrix}^T$ and $y \\in \\{0, 1\\}$. \n","\n","Positive and negative samples are stored in `data/positives.{trn, val, tst}` and `data/negatives.{trn, val, tst}` files, respectively. `*.trn`, `*.val`, `*.tst` contains the training, validation and test set, respectively.\n","\n","**Task:** **(Total: 1 pts)**\n","\n","* Finish `DataLoader` class which takes paths to the files to positive and negative samples, respectively. (Hint: use `np.loadtxt`) \n","\n","* Extract only the **6th** feature of the samples ($x_6$) (Index of the feature is stored in `FOI` variable). \n","\n","* Store the 6th feature of the positive samples in `pos` variable member (`pos` should have a shape $(N_{pos},)$). **(0.25 pts)**\n","\n","* Store the 6th feature of the negative samples in `neg` variable member (`neg` should have a shape $(N_{neg},)$). **(0.25 pts)**\n","\n","* Concatenate all samples (`pos`, `neg`) into `xs` resulting a vector with $(N_{pos}+N_{neg},)$ shape **(0.25 pts)**\n","\n","* `targets` should contain labels of the samples, where 1 and 0 correspond to a positive and negative sample, respectively. Thus `targets` have a shape $(N_{pos}+N_{neg},)$. **(0.25 pts)**"]},{"cell_type":"code","metadata":{"id":"uPS3YZEcizWx"},"source":["FOI = 5     # Feature of interest\n","\n","class DataLoader:\n","    def __init__(self, pos_path: str, neg_path: str):\n","        \"\"\"\n","        :param pos_path: Filepath to a \"positives.*\" file.\n","        :param neg_path: Filepath to a \"negatives.*\" file\n","        \"\"\"\n","\n","        #######################\n","        #### TODO YOUR CODE ###\n","        #######################\n","        self.pos = None\n","        self.neg = None\n","        self.xs = None\n","        self.targets = None\n","\n","train_set = DataLoader(pos_path=\"data/positives.trn\", neg_path=\"data/negatives.trn\")\n","val_set = DataLoader(pos_path=\"data/positives.val\", neg_path=\"data/negatives.val\")\n","test_set = DataLoader(pos_path=\"data/positives.tst\", neg_path=\"data/negatives.tst\")\n","\n","print(f\"Training set size: {train_set.xs.shape}\")\n","print(f\"Training target set size: {train_set.targets.shape}\")\n","print(f\"Training positives set size: {train_set.pos.shape}\")\n","print(f\"Training negatives set size: {train_set.neg.shape}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nNk6Vu1oECtC"},"source":["## Plotting the data \n","\n","**Task:** **(Total: 1 pts)**\n","\n","* Implement function for plotting two separate distributions of positive and negative samples feature using histogram (Use `plt.hist`, normalize the histograms by setting `density=true`.) Set `alpha=0.5`. \n","* Display the legend, so it's clear which distribution belongs to which label (positive, negative)."]},{"cell_type":"code","metadata":{"id":"g16Zhfx1ECMP"},"source":["def plot_data(pos: np.array, neg: np.array):\n","    \"\"\"\n","    :param pos: Positive samples\n","    :param neg: Negative samples\n","    \"\"\"\n","\n","    #######################\n","    #### TODO YOUR CODE ###\n","    #######################\n","    pass\n","\n","plot_data(pos=train_set.pos, neg=train_set.neg)\n","plot_data(pos=val_set.pos, neg=val_set.neg)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xzxEI0CmGNEd"},"source":["## Metrics\n","\n","In this part of the assigment you will implement functions which computes accuracy and F1 score given groundtruth and predicted labels.\n","\n","### Accuracy\n","\n","Accuracy is defined as follows:\n","$$Accuracy = \\frac{\\textrm{Number of correct predictions}}{\\textrm{Total number of predictions}}$$\n","\n","### F1 score\n","F1 score is computed using precision and recall metrics, which are defined as follows:\n","$$precision = \\frac{TP}{TP + FP}$$\n","\n","$$recall = \\frac{TP}{TP + FN}$$\n","where TP, FP, FN stand for true positive, false positive and false negative samples, respectively.\n","\n","$$\\textrm{F1 score} = \\frac{2\\cdot precision\\cdot recall}{precision + recall}$$\n","\n","**Task:** **(Total 0.5 pts)**\n","* Implement a function which computes accuracy given true labels and predicted labels.\n","\n","* Check the documentation of `sklearn.metrics.f1_score` (https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html)."]},{"cell_type":"code","metadata":{"id":"b2uBN6f3GMsH"},"source":["def accuracy(y_true: np.array, y_pred: np.array) -> float:\n","    \"\"\"\n","    :param y_true: Ground truth labels.\n","    :param y_pred: Predicted labels.\n","    :return: Accuracy of the predictions.\n","    \"\"\"\n","\n","    #######################\n","    #### TODO YOUR CODE ###\n","    #######################\n","    pass"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2nLGSFyKKur6"},"source":["## Thresholding\n","\n","Since $x_6 \\in [0, 1]$, a simple model can be created as follows:\n","\n","$$\\hat{y} = \\begin{cases}0, &\\text{if}\\ x_6 < T\\\\ 1, & \\text{otherwise} \\end{cases}$$\n","\n","where $T$ is hyperparameter representing a threshold.\n","\n","**Task:** **(Total: 2.5 pts)**\n","\n","* Implement `threshold` function which for given threshold `T` and feature `feature` computes the prediction $\\hat{y}$. **(0.5 pts)**\n","* Given a list of thresholds `thresholds` compute the accuracy and F1 score for each threshold on the training set. Use `sklearn.metrics.f1_score` and previously implemented `accuracy`. **(0.5 pts)**\n","* Which threshold would you select and why? (Just by looking at the plots, no need to run optimization).  **(0.5 pts)**\n","* Answer below why is F1 score better metric than accuracy in this particular case? **(1 pts)**\n","\n","**Answer:** TODO answer"]},{"cell_type":"code","metadata":{"id":"UfcA6rC_OGg6"},"source":["thresholds = np.linspace(start=0., stop=1., num=20)\n","\n","def threshold(feature: np.array, T: float) -> np.array:\n","    \"\"\"\n","    :param feature: Using given feature a sample label is predicted.\n","    :param T: Threshold which is used to separate classes.\n","    \"\"\"\n","\n","    #######################\n","    #### TODO YOUR CODE ###\n","    #######################\n","    pass\n","\n","#######################\n","#### TODO YOUR CODE ###\n","#######################\n","accuracies = None\n","f1_scores = None\n","\n","\n","plt.plot(thresholds, accuracies, label=\"Accuracy\")\n","plt.plot(thresholds, f1_scores, label=\"F1 Score\")\n","plt.legend()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rMIWp644jv-n"},"source":["## Baseline classifier\n","\n","Baseline classifier ignores sample feature and returns random probability, that a sample belongs to the class 1 (positive). (Hint probability is in range $[0, 1]$. Use `np.random.random`.)\n","\n","**Task:** **(Total: 0.5 pts)**\n","* Finish the implementation of the `RandomClassifier`."]},{"cell_type":"code","metadata":{"id":"CB1hnmoUk2m7"},"source":["class BaseClassifier:\n","    def prob_class_1(self, xs: np.array) -> np.array:\n","        \"\"\"\n","        :param xs: Input samples features.\n","        :return: Probability of a sample belonging to a class \"1\" (positive).\n","        \"\"\"\n","\n","        raise NotImplemented(\"prob_class_1 method not implemented.\")\n","\n","class RandomClassifier(BaseClassifier):\n","    def __init__(self):\n","        super().__init__()\n","\n","    def prob_class_1(self, xs: np.array) -> np.array:\n","        #######################\n","        #### TODO YOUR CODE ###\n","        #######################\n","        pass"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zQdgx9ALRGkV"},"source":["## Generative classifier\n","\n","In this part you will create generative classifiers based on gaussian probability distribution.\n","\n","Generative classifier tries to model the distributions of each class. Given the data, mean $\\mu$ and standard deviation $\\sigma$ of a gaussian distribution can be estimated as follows:\n","\n","$$\\mu = E[\\mathbf{X}] = \\frac{1}{N} \\sum_{i=1}^N X_i$$\n","\n","$$\\sigma = \\sqrt{E[(\\mathbf{X} - \\mu)^2]}$$\n","\n","where $N$  represents the number of samples $\\mathbf{X}$. $E[.]$ computes the average value of a given vector.\n","\n","**Task:** **(Total: 1 pts)**\n","* Compute the mean and standard deviation for each class (positive, negative) in the training set. You can use `np.mean`, Don|t use `np.std`.\n"]},{"cell_type":"code","metadata":{"id":"rYoJtzDDipdI"},"source":["#######################\n","#### TODO YOUR CODE ###\n","#######################\n","mean_pos = None\n","std_pos = None\n","mean_neg = None\n","std_neg = None\n","\n","print(f\"Mean positive: {mean_pos}\")\n","print(f\"Std. positive: {std_pos}\")\n","print(f\"Mean negative: {mean_neg}\")\n","print(f\"Std. negative: {std_neg}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cX1-tS4hp3LV"},"source":["### Plotting the estimated distributions\n","\n","**Task:** **(Total: 1 pts)**\n","* From the estimated parameters create the gaussian distributions using `scipy.stats.norm(loc, scale)` where `loc` and `scale` represents $\\mu$ and $\\sigma$, respectively. **(0.5 pts)**\n","\n","* On top of the data distributions plotted using `plot_data`, plot the modeled gaussian distributions. Set range of x axis to $[-0.5, 1.5]$ (Sample x in that range as well). Sample the distributions using the method `.pdf(x)` of `scipy.stats.norm`. **(0.5 pts)**"]},{"cell_type":"code","metadata":{"id":"B_S8ins5rOXU"},"source":["#######################\n","#### TODO YOUR CODE ###\n","#######################\n","\n","x = None    # x values at which the distributions will be sampled.\n","norm_dist_pos = None\n","norm_dist_neg = None"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mHMnux7yscGJ"},"source":["### Computing the posterior probability\n","\n","Posterior probability (sample belonging to the class $c$ given data $x$) can be computed using the Bayes rule:\n","\n","$$P(y = c | x) \\frac{P(x|y=c)P(y=c)}{\\sum_{c'}P(x|y=c')P(y=c')}$$ \n","\n","where $P(x|y = c)$ are the previously estimated distributions. $P(y=c)$ is the class prior probability.\n","\n","The constructor of `GenerativeClassifier`takes the estimated normal distributions of samples in the training set and the prior probabilities.\n","\n","For more details: https://medium.com/swlh/understanding-gaussian-classifier-6c9f3452358f\n","\n","\n","**Task:** **(Total: 1 pts)**\n","* Implement the `prop_class_1` method, which returns the probability, that a sample belongs to the class 1 (positive). (Hint: use `.pdf` for sampling $P(x|y=c)$.)"]},{"cell_type":"code","metadata":{"id":"q1nmXAlTu7ST"},"source":["class GenerativeClassifier(BaseClassifier):\n","    def __init__(self, pos_norm, neg_norm, pos_prior: float, neg_prior: float):\n","        super().__init__()\n","\n","        self.pos_norm = pos_norm\n","        self.neg_norm = neg_norm\n","        self.pos_prior = pos_prior\n","        self.neg_prior = neg_prior\n","\n","    def prob_class_1(self, xs: np.array) -> np.array:\n","        #######################\n","        #### TODO YOUR CODE ###\n","        #######################\n","        pass"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M-HD8teiwvp0"},"source":["**Task:** **(Total: 1.5 pts)**\n","* Create a generative classifier `generative_classifier_flat_prior` with priors $P(y=1) = 0.5, P(y=0) = 0.5$. **(0.5 pts)**\n","\n","* Create a generative classifier `generative_classifier_full_prior` with priors $P(y=1) = 0.2, P(y=0) = 0.8$. **(0.5 pts)**\n","\n","* Plot both posterior probabilities over the train data histogram. Set range of x axis to $[-0.5, 1.5]$. (Hint: Sample the probabilities using `x`) **(0.5 pts)**"]},{"cell_type":"code","metadata":{"id":"njRiy93HwG_B"},"source":["x = np.linspace(start=-0.5, stop=1.5)\n","\n","#######################\n","#### TODO YOUR CODE ###\n","#######################\n","generative_classifier_flat_prior = None\n","generative_classifier_full_prior = None"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RgaOIaXhxpuA"},"source":["## Evaluation\n","\n","Given the `thresholds` threshold the probabilities and evaluate `RandomClassifier` and `GenerativeClassifier` (`generative_classifier_full_prior` as well as `generative_classifier_flat_prior`) using accuracy and F1 score on the **test set**.\n","\n","**Task:** **(Total: 2 pts)**\n","* Compute accuracies and F1 scores for `RandomClassifier` and `GenerativeClassifier` for each threshold. **(1 pts)**\n","\n","* Plot computed the accuracies on the y-axis with `thresholds` on the x-axis. **(0.5 pts)**\n","\n","\n","* Plot computed the F1 scores on the y-axis with `thresholds` on the x-axis. **(0.5 pts)**"]},{"cell_type":"code","metadata":{"id":"obiQgzElzCic"},"source":["#######################\n","#### TODO YOUR CODE ###\n","#######################"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a8GMzkJn2X3x"},"source":["## Polynomial Regression\n","\n","Given data $y = sin(x)$, a noise will be added to the training data resulting in $y' = sin(x) + noise$.\n","\n","Retrieve the parameters of a polynomial model using `np.polyfit`, given the model parameters a model can be constructed using `np.poly1d`.\n","\n","See: https://numpy.org/doc/stable/reference/generated/numpy.poly1d.html\n","https://numpy.org/doc/stable/reference/generated/numpy.polyfit.html\n","https://www.w3schools.com/python/python_ml_polynomial_regression.asp\n","\n","**Task:** **(Total 2.5 pts)**\n","* Implement a model learning the function $y$ given only $y'$ (`y_train`). **(0.5 pts)**\n","* Create two polynomial models with the **polynomial degree 3 and 15**. **(0.5 pts)**\n","* Using the test set compute and print the MSE error for each model - $MSE = \\frac{1}{N} \\sum(y - \\hat{y})^2$, where $\\hat{y}$, $N$ stands for predicted value and number of samples, respectively. **(0.5 pts)**\n","* Answer below, which model is better and why. **(1 pts)**\n","\n","**Answer:** TODO answer"]},{"cell_type":"code","metadata":{"id":"v78d2FD54Vdb"},"source":["from math import pi\n","\n","x = np.linspace(start=0, stop=2 * pi, num=100)\n","x_train = x[::2]\n","x_test = x[1::2]\n","\n","noise = np.random.randn(*x_train.shape) * 0.2\n","y_train = np.sin(x_train) + noise\n","y_test = np.sin(x_test)\n","\n","#######################\n","#### TODO YOUR CODE ###\n","#######################\n","reg_model_deg_3 = None\n","reg_model_deg_15 = None\n","\n","# plot model's learnt function\n","plt.plot(x_test, reg_model_deg_3(x_test), label=\"Deg: 3\")\n","plt.scatter(x_test, y_test, marker=\"+\", s=10, label=\"Test data\", c='#d62728')\n","plt.scatter(x_train, y_train, marker=\"+\", s=10, label=\"Train data\", c='#bcbd22')\n","plt.legend()\n","plt.show()\n","\n","plt.plot(x_test, reg_model_deg_15(x_test), label=\"Deg: 15\")\n","plt.scatter(x_test, y_test, marker=\"+\", s=10, label=\"Test data\", c='#d62728')\n","plt.scatter(x_train, y_train, marker=\"+\", s=10, label=\"Train data\", c='#bcbd22')\n","plt.legend()"],"execution_count":null,"outputs":[]}]}