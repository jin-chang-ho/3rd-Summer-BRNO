{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BISSIT 2021 NLP lab students.ipynb의 사본","provenance":[{"file_id":"1ypWpH-piF7z6ofoUnGldX5wYDfY_PdUk","timestamp":1626919037356}],"collapsed_sections":["SVQbIeSZDZna","Iy5mUUXqEDsO"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"d-3Wqes_CfSL"},"source":["# NLP - Sentiment Classification\n","\n","## Introduction\n","In this excercise, you will get aquainted with:\n","\n","0. Data preprocessing\n","1. Word Embeddings\n","2. Sentiment Classification/Analysis\n","\n","###About limits of Google Colab:   \n","The 12-hour limit is for a continuous assignment of VM. It means we can use GPU compute even after the end of 12 hours by connecting to a different VM.\n","\n","### Author:\n","* Jan Kohút (ikohut@fit.vutbr.cz)\n"]},{"cell_type":"markdown","metadata":{"id":"SVQbIeSZDZna"},"source":["## Dependencies\n","* *torch* – automatic gradient derivation, loss optimization, pre-implemented model primitives\n","* *torchtext* – for easy processing of text data\n","* *spacy* – text preprocessing\n","* *matplotlib* – graph visualisation\n"]},{"cell_type":"markdown","metadata":{"id":"Iy5mUUXqEDsO"},"source":["## Runtime type\n","\n","* First, set the runtime type to GPU: Runtime -> Change runtime type -> under Hardware accelerator chose GPU"]},{"cell_type":"markdown","metadata":{"id":"Cqc2CyXDEhMu"},"source":["## Checking for GPU"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zySj2gRTEn6n","executionInfo":{"status":"ok","timestamp":1626868639121,"user_tz":-120,"elapsed":222,"user":{"displayName":"Jan Kohút","photoUrl":"","userId":"02882781311835582045"}},"outputId":"0a84dd5d-9ea6-428c-c446-7ddb72a3d7e8"},"source":["import torch\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","if device.type == 'cuda':\n","  print(torch.cuda.get_device_name(0))\n","  !nvidia-smi\n","else:\n","  print(\"GPU is not availible: Runtime -> Change runtime type -> GPU\")\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Tesla T4\n","Wed Jul 21 11:57:18 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 470.42.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   69C    P0    31W /  70W |   1126MiB / 15109MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"WAJHwAgsE5jc"},"source":["## Easy progress bar for visualization\n"," \n","Whenever some data are processed you will see a progress bar, you can decide if you are willing to wait for a long time and preprocess a larger amount of dataset. "]},{"cell_type":"code","metadata":{"id":"Q899JQVSYLye"},"source":["from IPython.display import HTML, display\n","import time\n","\n","def progress(value, max=100):\n","    return HTML(\"\"\"\n","        <progress\n","            value='{value}'\n","            max='{max}',\n","            style='width: 100%'\n","        >\n","            {value}\n","        </progress>\n","    \"\"\".format(value=value, max=max))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rkOgV-v8Ft14"},"source":["## Preprocessing of data\n","\n","Most of the text datasets need to be preprocessed in some way. Often there are characters/words that are not useful for Sentiment Classification. Some words may be prefixed or suffixed with unnecessary characters.\n","\n","For this exercise, we simply replace or multiple white spaces with a single space. Also, we discard quotations. You can simply add other filters."]},{"cell_type":"code","metadata":{"id":"S7CuM77-pjfD"},"source":["# Replace all multiple spaces, new lines, and tabs with a single space and remove quotations\n","# These filters are specifically designed for the Yelp datasets\n","# You can add other filters (and experiment with different datasets)\n","\n","import re\n","\n","def preprocess_line(line):\n","  preprocessed_line = line\n","  preprocessed_line = re.sub(r'( |(\\\\n)|\\t)+', ' ', preprocessed_line)\n","  preprocessed_line = re.sub(r'\\\\\"', '', preprocessed_line)\n","  return preprocessed_line"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1ou7mAXUx0fi"},"source":["# Takes PyTorch dataset and returns three lists: labels, original lines, and preprocessed lines\n","# You can choose how much data you want (to speed up the preprocessing)\n","\n","def preprocess_data(data, max_size=2000):\n","  labels = []\n","  lines = []\n","  lines_preprocessed = []\n","\n","  if len(data) < max_size:\n","    max_size = len(data)\n","  progress_bar = display(progress(1, max_size), display_id=True)\n","\n","  for i, (label, line) in enumerate(data):\n","    labels.append(label)\n","    lines.append(line)\n","    lines_preprocessed.append(preprocess_line(line))\n","    progress_bar.update(progress(i, max_size))\n","    if i == max_size - 1:\n","      break\n","\n","  return labels, lines, lines_preprocessed"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"p5LHcEG4YBeV"},"source":["# Calls data preprocessing for training and testing part of the dataset\n","\n","def preprocess_dataset(trn_dataset, tst_dataset, trn_max_size=2000, tst_max_size=1000):\n","  trn_labels, trn_lines, trn_lines_preprocessed = preprocess_data(trn_dataset, max_size=trn_max_size)\n","  tst_labels, tst_lines, tst_lines_preprocessed = preprocess_data(tst_dataset, max_size=tst_max_size)\n","  return trn_labels, trn_lines, trn_lines_preprocessed, tst_labels, tst_lines, tst_lines_preprocessed"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9A1_pj8dHou8"},"source":["### Choosing your dataset\n","\n","Now, you can choose the dataset you want to experiment with. Theoretically, all should work, but I tested it only with the YelpReviewFull dataset.\n","\n","Typically, these datasets contain lines of text with assign labels. For example, the YelpReviewFull dataset contains reviews of Restaurants, Dentists, Bars, Beauty Salons, Doctors with rating/stars/labels (1, 2, 3, 4, or 5).\n","\n","You can later return and choose a different dataset (and do the whole exercise/clicking again). Don't be afraid. It's pretty fast, once you are done the first time. \n","\n","**For now, choose only one dataset and proceed.**"]},{"cell_type":"code","metadata":{"id":"4-mplANFhotf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626866784695,"user_tz":-120,"elapsed":15962,"user":{"displayName":"Jan Kohút","photoUrl":"","userId":"02882781311835582045"}},"outputId":"a42c652e-099f-431e-81f0-2f13755c7d34"},"source":["from torchtext.datasets import YelpReviewFull\n","pytorch_dataset = YelpReviewFull(root='.data', split=('train', 'test'))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["yelp_review_full_csv.tar.gz: 196MB [00:01, 154MB/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"-QYlu6UjhDBe"},"source":["from torchtext.datasets import YelpReviewPolarity\n","pytorch_dataset = YelpReviewPolarity(root='.data', split=('train', 'test'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s7urAHn9iRgJ"},"source":["from torchtext.datasets import AG_NEWS\n","pytorch_dataset = AG_NEWS(root='.data', split=('train', 'test'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7kgDBcJ9ia-J"},"source":["from torchtext.datasets import SogouNews\n","pytorch_dataset = SogouNews(root='.data', split=('train', 'test'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2mPAQEBniyF_"},"source":["from torchtext.datasets import YahooAnswers\n","pytorch_dataset = YahooAnswers(root='.data', split=('train', 'test'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kWKfG4KIi9aq"},"source":["from torchtext.datasets import AmazonReviewPolarity\n","pytorch_dataset = AmazonReviewPolarity(root='.data', split=('train', 'test'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jnlCRjlpjG28"},"source":["from torchtext.datasets import AmazonReviewFull\n","pytorch_dataset = torchtext.datasets.AmazonReviewFull(root='.data', split=('train', 'test'))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n1bV2N6PIwh7"},"source":["### Size of the dataset\n","\n","Now, if you have a lot of time you can preprocess a lot of data. The benefit is that the final neural network will be more accurate. The downside: it is slow. But you can achieve sensible results with as little data as 10000 lines, but if you have more time, use more."]},{"cell_type":"code","metadata":{"id":"MboxISgagcdV"},"source":["# Set the number of your training lines (reviews or whatever your lines represent)\n","trn_max_size = 10000\n","# Set the number of your testing lines (reviews or whatever your lines represent)\n","tst_max_size = 2000\n","\n","trn_labels, trn_lines, trn_lines_preprocessed, tst_labels, tst_lines, tst_lines_preprocessed = preprocess_dataset(pytorch_dataset[0], pytorch_dataset[1], trn_max_size=trn_max_size, tst_max_size=tst_max_size)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ECerF6wMKufy"},"source":["Explore the returned data and the difference between lines and preprocessed lines.\n","\n"]},{"cell_type":"code","metadata":{"id":"CB8b4pOqK3o5"},"source":["print(trn_labels[25])\n","print(trn_lines[25])\n","print(trn_lines_preprocessed[25])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HK2b7uRpKAza"},"source":["### Tokenization\n","\n","Every line needs to be converted to tokens (usually corresponds to a word). So, for example: \"An exercise about sentiment classification.\" will be split to:\n","\n","* An \n","* exercise \n","* about \n","* sentiment \n","* classification\n","* .\n"]},{"cell_type":"code","metadata":{"id":"AkEIwjEFWtoK"},"source":["from torchtext.data.utils import get_tokenizer\n","\n","# Use the spaCy library for tokenization\n","def tokenize_lines(lines):\n","  tokens = []\n","  len_lines = len(lines)\n","\n","  progress_bar = display(progress(1, len_lines), display_id=True)\n","\n","  spacy_tokenizer = get_tokenizer(\"spacy\", language='en')\n","\n","  for i, line in enumerate(lines):\n","    tokens.append(spacy_tokenizer(line))\n","    progress_bar.update(progress(i, len_lines))\n","\n","  return tokens"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Rdtke_MlLXX6"},"source":["Run the tokenization of preprocessed lines."]},{"cell_type":"code","metadata":{"id":"IWv61yg16wTa"},"source":["trn_tokens = tokenize_lines(trn_lines_preprocessed)\n","tst_tokens = tokenize_lines(tst_lines_preprocessed)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MwoDpThlLnSN"},"source":["Explore the returned tokens and the difference to preprocessed lines."]},{"cell_type":"code","metadata":{"id":"jSPmgR-JL1Ig"},"source":["print(trn_lines_preprocessed[25])\n","print(trn_tokens[25])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HXRpigyQL_Jy"},"source":["### Word Embeddings\n","\n","Now, we need word embeddings. Word embeddings represent words (tokens) with vectors of the same dimensionality. For this exercise, we use GloVe https://nlp.stanford.edu/projects/glove/ pre-trained embeddings, specifically, its 6B variant with the dimensionality being 100.\n","\n","Again, you can use different pre-trained embeddings (either change the dimensionality, or change the GloVe for fastText, or something else), it's a matter of your choice. \n","\n","Beware, some libraries might be huge and you are limited (here on Colab) when it comes to memory."]},{"cell_type":"code","metadata":{"id":"gJ18cOia-Or0"},"source":["from torchtext.vocab import GloVe\n","# Download the Glove, 6B, 100D\n","Glove_6B = GloVe(name='6B', dim=100)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aiugSCdbDWwu"},"source":["# Converts tokens to word embeddings\n","def get_lines_word_embeddings_for_tokens(tokens, pretrained_word_embeddings):\n","  len_tokens = len(tokens)\n","  lines_word_embeddings = []\n","\n","  progress_bar = display(progress(1, len_tokens), display_id=True)\n","  \n","  for i, line_tokens in enumerate(tokens):\n","      line_word_embeddings = pretrained_word_embeddings.get_vecs_by_tokens(line_tokens, lower_case_backup=True)\n","      lines_word_embeddings.append(line_word_embeddings)\n","          \n","      progress_bar.update(progress(i, len_tokens))\n","  \n","  return lines_word_embeddings"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0OJWlaBTOJGx"},"source":["Convert each token to corresponding word embedding."]},{"cell_type":"code","metadata":{"id":"TXeoDWWMFLVw"},"source":["trn_lines_word_embeddings = get_lines_word_embeddings_for_tokens(trn_tokens, Glove_6B)\n","tst_lines_word_embeddings = get_lines_word_embeddings_for_tokens(tst_tokens, Glove_6B)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xm5Uq44-OnCJ"},"source":["Explore the returned embeddings and the difference to tokens."]},{"cell_type":"code","metadata":{"id":"ZRzZfC18OoUf"},"source":["# Each token is now represented with a vector of the same dimensionality\n","print(trn_tokens[20])\n","print(trn_lines_word_embeddings[20])\n","print(trn_lines_word_embeddings[20].shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_m2XJ2nWPLZq"},"source":["# Print a specific word with its corresponding token\n","print(trn_tokens[20][1])\n","print(trn_lines_word_embeddings[20][1])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AHjex27jPb_U"},"source":["These pre-trained word embeddings have actual semantic meaning, so by adding and substracting them, you can get sensible results. For example: King + (Woman - Man) = Result. The nearest vector to the Result vector should be a vector that represents Queen."]},{"cell_type":"markdown","metadata":{"id":"qDjx9T_9QMt_"},"source":["As you might guess, some words/tokens do not have corresponding word embeddings in the pre-trained word embeddings library. PyTorch assigns 0 vectors to such words/tokens."]},{"cell_type":"code","metadata":{"id":"2fBwVQrxH37u"},"source":["# Counts the number of zero vectors (word embeddings), returns also the total\n","def get_number_of_zero_word_embeddings(lines_word_embeddings):\n","  len_word_embeddings = len(lines_word_embeddings)\n","  number_of_zero_word_embeddings = 0\n","  number_of_word_embeddings = 0\n","  \n","  progress_bar = display(progress(1, len_word_embeddings), display_id=True)\n","  \n","  for i, line_word_embeddings in enumerate(lines_word_embeddings):\n","    for e in line_word_embeddings:\n","      if e.sum() == 0:\n","        number_of_zero_word_embeddings += 1\n","      number_of_word_embeddings += len(line_word_embeddings)\n","\n","    progress_bar.update(progress(i, len_word_embeddings))\n","  \n","  return number_of_word_embeddings, number_of_zero_word_embeddings\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FO8_A9yrQ2PA"},"source":["Count the zero word embeddings to check if we have a sensible dataset."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":51},"id":"Px9FR-i1HcW4","executionInfo":{"status":"ok","timestamp":1626869430731,"user_tz":-120,"elapsed":57251,"user":{"displayName":"Jan Kohút","photoUrl":"","userId":"02882781311835582045"}},"outputId":"61f7d60b-7227-43f4-b38e-d54d8b8515ad"},"source":["trn_number_of_word_embeddings, trn_number_of_zero_word_embeddings = get_number_of_zero_word_embeddings(trn_lines_word_embeddings)\n","tst_number_of_word_embeddings, tst_number_of_zero_word_embeddings = get_number_of_zero_word_embeddings(tst_lines_word_embeddings)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","        <progress\n","            value='9999'\n","            max='10000',\n","            style='width: 100%'\n","        >\n","            9999\n","        </progress>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["\n","        <progress\n","            value='1999'\n","            max='2000',\n","            style='width: 100%'\n","        >\n","            1999\n","        </progress>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"piIgoLb9ReWp"},"source":["Print the fraction of zero word embeddings in our dataset, it should be marginal."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IkumlPxlKbzN","executionInfo":{"status":"ok","timestamp":1626869433011,"user_tz":-120,"elapsed":204,"user":{"displayName":"Jan Kohút","photoUrl":"","userId":"02882781311835582045"}},"outputId":"8023fe15-5e4e-4a2b-fc58-620ab9d836a3"},"source":["print(\"TRN {:.10f}%\".format((trn_number_of_zero_word_embeddings / float(trn_number_of_word_embeddings)) * 100))\n","print(\"TST {:.10f}%\".format((tst_number_of_zero_word_embeddings / float(tst_number_of_word_embeddings)) * 100))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["TRN 0.0021970226%\n","TST 0.0020701577%\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"mG3_beu0R9SA"},"source":["## Building the dataset \n","\n","Now that we've preprocessed the data, we need to create a data loader that will serve for the training and testing of the neural network. "]},{"cell_type":"markdown","metadata":{"id":"p-ZWJeUWSU9g"},"source":["### SentimentDataset class\n","\n","To achieve batch training of the neural network (and therefore effective parallelization on GPU) we need lines (reviews or whatever it is for you) to have the same lengths. This class allows you to pick the length of the lines/sequences/reviews/ which is the number of words/number of tokens. Lines that are longer are simply cropped, shorter lines are padded with zero vectors.\n","\n","The class (object of this class) also returns weights for each word/token in the sequence. The weights are the same for all words, but zero for the padded vectors."]},{"cell_type":"code","metadata":{"id":"6t6-rbvXLk_9"},"source":["from torch.utils.data import Dataset\n","from torch.nn.functional import pad\n","from torch import ones\n","\n","class SentimentDataset(Dataset):\n","\n","    def __init__(self, labels, lines_word_embeddings, words_per_line=350):\n","        self.labels = labels\n","        self.lines_word_embeddings = lines_word_embeddings\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","    def crop_and_pad_line_word_embeddings_with_zero_word_embeddings(self, line_word_embeddings):\n","      if line_word_embeddings.shape[0] < words_per_line:\n","        pad_size = words_per_line - line_word_embeddings.shape[0]\n","        padded_line_word_embeddings = pad(line_word_embeddings, (0, 0, 0, pad_size), mode='constant', value=0)\n","        padded_line_word_weights = pad(ones(line_word_embeddings.shape[0], 1)/line_word_embeddings.shape[0], (0, 0, 0, pad_size), mode='constant', value=0)\n","        return padded_line_word_embeddings, padded_line_word_weights      \n","      else:\n","        padded_line_word_weights = ones(words_per_line, 1)/words_per_line\n","        return line_word_embeddings[:words_per_line], padded_line_word_weights\n","\n","    def __getitem__(self, idx):\n","      crop_and_pad_line_word_embedding, padded_line_word_weights = self.crop_and_pad_line_word_embeddings_with_zero_word_embeddings(self.lines_word_embeddings[idx])\n","      return {'labels': self.labels[idx] - 1, 'lines_word_embeddings': crop_and_pad_line_word_embedding, 'lines_word_weights': padded_line_word_weights}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UEs-9gfATzKT"},"source":["Create PyTorch Dataset for Sentiment Classification."]},{"cell_type":"code","metadata":{"id":"-I6M-HaLP-ee"},"source":["# You can set the words per line here, the longer the sequence, the slower will be the training/inference of the neural network.\n","words_per_line = 350\n","\n","trn_dataset = SentimentDataset(trn_labels, trn_lines_word_embeddings, words_per_line=words_per_line)\n","tst_dataset = SentimentDataset(tst_labels, tst_lines_word_embeddings, words_per_line=words_per_line)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-QVQYHHLUTVR"},"source":["Create PyTorch DataLoader for Sentiment Classification."]},{"cell_type":"code","metadata":{"id":"c35-QHonaQ2X"},"source":["from torch.utils.data import DataLoader\n","\n","# Choose your batch size\n","batch_size = 32\n","\n","trn_data_loader = DataLoader(trn_dataset, batch_size=batch_size, shuffle=True)\n","tst_data_loader = DataLoader(tst_dataset, batch_size=batch_size, shuffle=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Kah74w_4d375"},"source":["### SentimentNet class (The neural network for sentiment classification)\n","\n","The computational graph (illustration) of the net can be found here: https://drive.google.com/file/d/1bS_3QjzD-uVvD45uj2mfPY2TROJfR3V5/view?usp=sharing\n","\n","$$\n","core\\_net = W_1 * tanh((W_0 * E) + b_0))\n","$$\n","\n","$$\n","net = SoftMax(AVG(core\\_net))\n","$$\n","\n"]},{"cell_type":"markdown","metadata":{"id":"sLovsh92fRd-"},"source":["# Your task\n","\n","Your goal is to implement the *core_net* according to the given illustration and mathematical expression.\n","\n","#### Tips\n","* Use two nn.Linear layers, one with the bias set to True (b_0), the other with the bias set to false (just W_1)\n","* Use nn.Tanh\n","* Basically an affine transformation followed by nonlinearity and final linear transformation\n","* Feel free to experiment with more complex (or simpler models)\n","\n","#### Dimensions\n","* E_dim is the word embedding dimension\n","* I_dim is the dimension of the vectors after the first affine transformation (after the first nn.Linear with the bias set to True (W_0, b_0))\n","* C is the number of the classes (5 for YelpReviewFull) and the dimensionality of the vectors after the linear transformation (second nn.Linear (W_1))"]},{"cell_type":"markdown","metadata":{"id":"i5l6aYLFhTLs"},"source":["### Averaging and training\n","\n","The output of *core_net* is averaged based on the weights provided by the dataset (dataloader), remember, the zero-padding vectors have zero weights and therefore have no effect when it comes to training.\n","\n","The averaged outputs of *core_net* are fed to SoftMax and the Net as a whole is trained with the standard CrossEntropy loss."]},{"cell_type":"code","metadata":{"id":"Vs56g2zUvvbU"},"source":["from torch import nn\n","from torch import sum\n","\n","class SentimentNet(nn.Module):\n","    def __init__(self, E_dim, I_dim, C):\n","        super(SentimentNet, self).__init__()\n","        self.E_dim = E_dim\n","        self.I_dim = I_dim\n","        self.C = C\n","        #nn.Linear\n","        #nn.Tanh\n","        #nn.Linear\n","\n","    def forward(self, line_word_embeddings, line_word_weights):\n","        x = line_word_embeddings\n","        w = line_word_weights\n","        \n","        #core_net = call first nn.Linear\n","        #core_net = call nn.Tanh\n","        #core_net = call secod nn.Linear\n","\n","        net = core_net * w\n","        net = sum(net, dim=1)\n","\n","        return net, core_net"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6d0Srwdxjj5-"},"source":["Before training, check the GPU again."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2qA7hHtI_Pqp","executionInfo":{"status":"ok","timestamp":1626867227511,"user_tz":-120,"elapsed":194,"user":{"displayName":"Jan Kohút","photoUrl":"","userId":"02882781311835582045"}},"outputId":"ad3c2f26-24e4-4b0f-d468-5f0780da8da0"},"source":["import torch\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","if device.type == 'cuda':\n","  print(torch.cuda.get_device_name(0))\n","  !nvidia-smi\n","else:\n","  print(\"GPU is not availible: Runtime -> Change runtime type -> GPU\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Tesla T4\n","Wed Jul 21 11:33:47 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 470.42.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   40C    P8     9W /  70W |      3MiB / 15109MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"kFPhnf3gjsPV"},"source":["###SentimentTrainWrapper class \n","\n","A helper class for training that provides standard methods as:\n","* train_step - makes one training step (forward pass, backward pass, update of the net parameters)\n","* test_step - makes one testing step (forward pass)\n"]},{"cell_type":"code","metadata":{"id":"W2fdx1oopOKH"},"source":["import sys\n","from torch.nn import Softmax\n","from torch import sum\n","\n","class SentimentTrainWrapper:\n","    def __init__(self, E_dim, I_dim, C, learning_rate):\n","      self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","      if device.type != 'cuda':\n","        print(\"GPU is not availible: Runtime -> Change runtime type -> GPU\")\n","        sys.exit(1)\n","      self.net = SentimentNet(E_dim, I_dim, C)\n","      self.net = self.net.to(self.device)\n","      self.loss = torch.nn.CrossEntropyLoss().to(self.device)\n","      self.optimizer = torch.optim.Adam(self.net.parameters(), lr=learning_rate)\n","\n","    def train_step(self, batch):\n","        self.optimizer.zero_grad()\n","        output, loss = self.forward_pass(batch)\n","        loss.backward()\n","        self.optimizer.step()\n","        return output, loss\n","\n","    def test_step(self, batch):\n","        output, loss = self.forward_pass(batch)\n","        return output, loss\n","\n","    def forward_pass(self, batch):\n","        labels = batch['labels'].to(self.device)\n","        lines_word_embeddings = batch['lines_word_embeddings'].to(self.device)\n","        lines_word_weights = batch['lines_word_weights'].to(self.device)\n","\n","        output, _ = self.net.forward(lines_word_embeddings, lines_word_weights)\n","        loss = self.loss(output, labels)\n","        return output, loss\n","\n","    def inference_pass(self, batch):\n","        lines_word_embeddings = batch['lines_word_embeddings'].to(self.device)\n","        lines_word_weights = batch['lines_word_weights'].to(self.device)\n","\n","        aggregated_output, output_per_word = self.net.forward(lines_word_embeddings, lines_word_weights)\n","        return aggregated_output, output_per_word\n","\n","    def count_accuracy(self, batch, output):\n","        labels = batch['labels'].to(self.device)\n","        predictions = torch.argmax(output, dim=1).to(self.device)\n","        return sum(labels == predictions) / batch['labels'].shape[0]\n","\n","    def count_probs(self, output, dim=1):\n","        return Softmax(dim=dim)(output)\n","\n","    def set_train(self):\n","        self.net = self.net.train()\n","\n","    def set_eval(self):\n","        self.net = self.net.eval()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zlxGxGlVkVfY"},"source":["Initialize the network."]},{"cell_type":"code","metadata":{"id":"xBfMI1KzGbQp"},"source":["# Set the appropriate dimensions\n","E_dim = 100\n","I_dim = 200 \n","C = 5\n","\n","# Choose your learning rate\n","learning_rate = 0.0003\n","\n","# Init the training and the net\n","sentiment_train_wrapper = SentimentTrainWrapper(E_dim, I_dim, C, learning_rate)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"37KGRLW1k3Xo"},"source":["Functions for testing the net."]},{"cell_type":"code","metadata":{"id":"7a6KfO02ISja"},"source":["def is_valid_iter(it_no, step):\n","  return it_no % step == 0\n","\n","def test(sentiment_train_wrapper, iteration, dataset, train):\n","    total_loss = 0\n","    total_accuracy = 0\n","    total_number_of_lines = 0\n","    start_time = time.time()\n","    total_net_time = 0\n","    total_images_area = 0\n","\n","    with torch.no_grad():\n","        for it_count, batch in enumerate(dataset, 1):\n","            net_t1 = time.time()\n","            outs, loss = sentiment_train_wrapper.test_step(batch)\n","            total_net_time += time.time() - net_t1\n","\n","            total_loss += loss.mean().item()\n","            total_accuracy += sentiment_train_wrapper.count_accuracy(batch, outs)\n","            total_number_of_lines += batch['lines_word_embeddings'].shape[0]\n","\n","            if train:\n","                if it_count > 1500 // dataset.batch_size:\n","                    break\n","\n","    end_time = time.time()\n","\n","    trn_tst = \"TST DATASET\"\n","    if train:\n","      trn_tst = \"TRN DATASET\"\n","\n","    print('{} loss:{:.5f} accuracy:{:.2f}% full_speed:{:.0f} net_speed:{:.0f} time:{:.1f}'.format(\n","           trn_tst,\n","           total_loss / it_count,\n","           (total_accuracy / it_count) * 100,\n","           total_number_of_lines / (end_time - start_time),\n","           total_number_of_lines / total_net_time,\n","           end_time - start_time))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"noGZGwAtlPdr"},"source":["### Training the network for sentiment classification\n","\n","The main training cycle. If you stop or rerun this cycle the previous model (with its weights) is still stored in the memory, so you will continue to finetune this model. To start from scratch you have to rerun the initialization of sentiment_train_wrapper.\n","\n","You can see if the network is performing better than \"guessing\" based on accuracy. If you have 5 classes, it should be better than 20% (on YelpFullReview you should get to 50% pretty quickly)."]},{"cell_type":"code","metadata":{"id":"_N8meQjhWJUZ"},"source":["# The number of trainig iterations\n","max_iterations = 5000\n","# The interval (in iterations) of printing training and testing statistics\n","test_step = 250\n","\n","iteration = 0\n","\n","total_number_of_lines = 0\n","trn_loss = 0.0\n","trn_accuracy = 0.0\n","\n","train_time = 0.0\n","train_net_time = 0.0\n","training_time = 0.0\n","\n","stop_training = False\n","\n","while True:\n","  for batch in trn_data_loader:\n","    show_test = is_valid_iter(iteration, test_step)\n","    show_train = is_valid_iter(iteration, test_step) and iteration > 0\n","    \n","    if show_test or show_train:\n","      print()\n","      print(\"ITERATION {}\".format(iteration))\n","      print(\"---------------------------------------------------------------------------------------------------\")\n","    \n","    # Test the network and show the results on training and testing dataset\n","    if show_test:\n","      if iteration > 0:\n","        train_time = time.time() - train_timer\n","      train_timer = time.time()\n","\n","      sentiment_train_wrapper.set_eval()\n","      test(sentiment_train_wrapper, iteration, trn_data_loader, True)\n","      test(sentiment_train_wrapper, iteration, tst_data_loader, False)\n","      sentiment_train_wrapper.set_train()\n","    \n","    # Show training statistics\n","    if show_train:\n","      training_time += train_time\n","      net_speed = (test_step * lines_word_embeddings.shape[0]) / train_net_time\n","      trn_loss /= test_step\n","      trn_accuracy /= test_step\n","      print(\"TRAIN {} ({:d}k lines seen) loss:{:.5f} accuracy:{:.2f}% time:{:.1f} net_speed:{}\".format(\n","            iteration,\n","            total_number_of_lines // 1000,\n","            trn_loss,\n","            trn_accuracy * 100,\n","            train_time,\n","            net_speed,\n","            ))\n","      print()\n","      train_net_time = 0.0\n","      trn_loss = 0.0\n","      trn_accuracy = 0.0\n","            \n","    if show_test or show_train:\n","      print(\"---------------------------------------------------------------------------------------------------\")\n","   \n","    if iteration == max_iterations:\n","      stop_training = True\n","      break\n","    \n","    # Extract the data from the batch\n","    labels = batch['labels']\n","    lines_word_embeddings = batch['lines_word_embeddings']\n","    lines_word_weights = batch['lines_word_weights']\n","            \n","    # The actual training\n","    start_of_the_train_step = time.time()\n","    outs, loss = sentiment_train_wrapper.train_step(batch)\n","    train_net_time += time.time() - start_of_the_train_step\n","    \n","    # Accumulate some statistics\n","    loss = loss.mean()\n","    accuracy = sentiment_train_wrapper.count_accuracy(batch, outs)\n","    trn_loss += loss\n","    trn_accuracy += accuracy \n","    total_number_of_lines += lines_word_embeddings.shape[0]\n","\n","    iteration += 1\n","\n","  if stop_training:\n","    break\n","\n","training_time += time.time() - train_timer\n","\n","print()\n","print(\"AVERAGE TIME OF 100 ITERATIONS: {}\".format((training_time / (max_iterations)) * 100))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"89YQzJp6m67E"},"source":["## Playing with the Sentiment Network\n","\n","Now we can observe if the network is doing something sensible. Simply assign your \"review\" into the my_review variable and proceed."]},{"cell_type":"code","metadata":{"id":"L1TSpoz_wb6u"},"source":["# Converts line of text into the input for the network (also returns the tokenized version of the input text line)\n","def convert_my_review_to_input_for_sentiment_net(review, pretrained_word_embeddings):\n","  my_review_preprocessed = preprocess_line(review)\n","  my_review_tokens = tokenize_lines([my_review_preprocessed])\n","  my_review_word_embeddings = get_lines_word_embeddings_for_tokens(my_review_tokens, pretrained_word_embeddings)[0]\n","\n","  my_review_batch = {}\n","  my_review_batch['lines_word_embeddings'] = my_review_word_embeddings.unsqueeze(0)\n","  my_review_batch['lines_word_weights'] = ones(1, my_review_word_embeddings.shape[0], 1)/my_review_word_embeddings.shape[0]\n","\n","  return my_review_batch, my_review_tokens[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fiQROlm81yJy"},"source":["# Write your review\n","my_review = \"I love this beautiful place!\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":51},"id":"I4z246plqhaZ","executionInfo":{"status":"ok","timestamp":1626873758502,"user_tz":-120,"elapsed":743,"user":{"displayName":"Jan Kohút","photoUrl":"","userId":"02882781311835582045"}},"outputId":"225c00aa-7c3f-482d-c45f-9d27c0a25b51"},"source":["# Preprocess your review for the net\n","input_for_sentiment_net, my_review_tokens = convert_my_review_to_input_for_sentiment_net(my_review, Glove_6B)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","        <progress\n","            value='0'\n","            max='1',\n","            style='width: 100%'\n","        >\n","            0\n","        </progress>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["\n","        <progress\n","            value='0'\n","            max='1',\n","            style='width: 100%'\n","        >\n","            0\n","        </progress>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"nryYzUhEsEQ_"},"source":["# Pass the review through the net\n","aggregated_output, per_word_output = sentiment_train_wrapper.inference_pass(input_for_sentiment_net)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Bv5F7rH4wB42"},"source":["# Convert the logits into probabilities with SoftMax\n","my_review_probs = sentiment_train_wrapper.count_probs(aggregated_output)\n","my_review_word_probs = sentiment_train_wrapper.count_probs(per_word_output, dim=2)\n","my_review_probs = my_review_probs[0, :].tolist()\n","my_review_word_probs = my_review_word_probs[0, :, :].tolist()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z8lTdURwo7Ty"},"source":["#### Print the results for the whole review"]},{"cell_type":"code","metadata":{"id":"Afx-7v-PzT4_"},"source":["for star, prob in enumerate(my_review_probs, 1):\n","  print(star, \"{:.2f}\".format(prob))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_9xmCM6CpJic"},"source":["#### Print the results for each word in the review\n"]},{"cell_type":"code","metadata":{"id":"fXv0XHQp1roz"},"source":["for word, probs in zip(my_review_tokens, my_review_word_probs):\n","  print(word)\n","  for star, prob in enumerate(probs, 1):\n","    print(star, \"{:.2f}\".format(prob))\n","  print()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B8bwzWR1pQaw"},"source":["#### Plot the results for each word in the review\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":669},"id":"u5nMm0XT6113","executionInfo":{"status":"ok","timestamp":1626873768503,"user_tz":-120,"elapsed":747,"user":{"displayName":"Jan Kohút","photoUrl":"","userId":"02882781311835582045"}},"outputId":"48cd7f59-c3c6-4a68-f73f-d6a8b4138bce"},"source":["import matplotlib.pyplot as plt\n","import matplotlib.pylab as pylab\n","import numpy as np\n","\n","params = {'legend.fontsize': 'xx-large',\n","          'figure.figsize': (15, 5),\n","         'axes.labelsize': 'xx-large',\n","         'axes.titlesize':'xx-large',\n","         'xtick.labelsize':'xx-large',\n","         'ytick.labelsize':'xx-large'}\n","pylab.rcParams.update(params)\n","\n","labels = my_review_tokens\n","probs = np.transpose(np.asarray(my_review_word_probs))\n","\n","width = 0.35   \n","\n","fig, ax = plt.subplots()\n","\n","fig.set_size_inches(22.5, 10.5)\n","\n","indexes = list(range(probs.shape[1]))\n","\n","bottom = np.zeros(probs.shape[1])\n","for star, p in enumerate(probs):\n","  ax.bar(indexes, p, width, bottom=bottom, label=star+1)\n","  bottom += p\n","\n","plt.xticks(indexes, labels, rotation=0)\n","\n","ax.set_xlabel('WORDS')\n","ax.set_ylabel('PROBS')\n","ax.set_title('RATING PER WORD')\n","\n","ax.legend()\n","\n","plt.show()\n"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 1620x756 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"s9bPm2D-pgMq"},"source":["##Contact\n","\n","If you have any questions or problems considering this exercise, you can write me an email: ikohut@fit.vutbr.cz"]}]}